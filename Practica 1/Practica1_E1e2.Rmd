---
title: 'Práctica 1: Aprendizaje Automático'
author: "Eloy Bedia García"
date: "10 de Marzo de 2018"
output: pdf_document
subtitle: "1. Ejercicios sobre la búsqueda iterativa de óptimos"
---

**EJERCICIO 1**

1. Implementar el algoritmo de gradiente descendente.
``` {r}
BatchGradientDescent = function(var, f, fderivadas, mu, epsilon,limit = Inf) {
  y <- var
  dim(y) <- dim(var)
  
  cat(var ," ", dim(var))

  iteraciones <- 0

  while(eval(f) > epsilon && iteraciones < limit) {
    for(i in 1:dim(var)) {
      y[i] <- var[i] - mu * eval(fderivadas[i])
    }

    var <- y
    cat(var , " " , y , "\n")
  
    
    iteraciones <- iteraciones + 1
  }
  
  list(y,iteraciones)
}
```

**EJERCICIO 2**

2. (2 puntos) Considerar la función $f(u,v) = (u^{3}e^{v-2} - 4v^{3}e^{-u})^{2}$. Usar gradiente descendente para encontrar un mínimo de esta función, comenzando desde el punto (u, v) = (1, 1) y usando una tasa de aprendizaje $\eta$ = 0,1.

``` {r echo=FALSE}
var <- c(0,0)
dim(var) <- 2
f <- expression( ( (var[1] ** 3) * (exp(var[2] - 2) )  - 4 * (var[2] ** 3) * exp(-var[1]) ) ** 2 )
fu <- expression(2 * ( 3 * (var[1] ** 2) * exp(var[2] - 2) + 4 * (var[2] ** 3) * exp(-var[1]) ) )
fv <- expression(2 * ( (var[1] ** 3) * exp(var[2] - 2) - 12 * (var[2] ** 2) * exp(-var[1]) ) )
fderivadas <- c(fu,fv)
```

``` {r}
var <- c(1,1)
dim(var) <- 2
mu <- 0.1
epsilon <- 10**-14

res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
punto <- res[[1]]
iteraciones <- res[[2]]
```

a) Calcular analíticamente y mostrar la expresión del gradiente de la función $f(u,v)$
$\frac{\partial (u^{3}e^{v-2} - 4v^{3}e^{-u})^{2}}{\partial u} = 2 ( 3u^{2} e^{v - 2}  + 4 v^{3} e^{-u} ) $
$\frac{\partial (u^{3}e^{v-2} - 4v^{3}e^{-u})^{2}}{\partial v} = 2 (u^{3}e^{v-2} - 12v^{2}e^{-u})$

b) Cuántas iteraciones tarda el algoritmo en obtener por primera vez un valor de $f(u,v)$ inferior a $10^{-14}$ .
``` {r echo = FALSE}
cat("Para al vanzar el minimo de la función f, el algoritmo utiliado invierte ", iteraciones)
```

c) ¿En qué coordenadas (u, v) se alcanzó por primera vez un valor igual o menor a $10^{-14}$  en el apartado anterior.
``` {r echo = FALSE}
var <- punto
cat("En el punto (",punto[1],",",punto[2],") = ",eval(f))
```


**EJERCICIO 3**

3. Considerar ahora la función $f(x,y) = (x-2)^{2} + 2(y+2)^{2} + 2sin(2\pi x)sin(2\pi y)$

```{r echo=FALSE}
var <- c(0,0)
dim(var) <- 2
f <- expression((var[1] - 2) **2 + 2 *(var[2] + 2) ** 2 + 2 * sin(2 *pi * var[1]) * sin(2 * pi * var[2]))
fu <- expression(2 * (var[1] - 2) + 2 * sin(2 * pi * var[2]) * cos(2 * pi * var[1]) * 2 * pi)
fv <- expression(4 * (var[2] + 2) + 2 * sin(2 * pi * var[1]) * cos(2 * pi * var[2]) * 2 * pi)
fderivadas <- c(fu,fv)
```

a) Usar gradiente descendente para minimizar esta función. Usar como punto inicial $(x_{0} = 1, y_{0} = 1)$, tasa de aprendizaje $\eta$ y un máximo de 50 iteraciones. Generar un gráfico de cómo desciende el valor de la función con las iteraciones. Repetir el experimento pero usando $\eta$ = 0,1, comentar las diferencias y su dependencia de $\eta$.

``` {r include = FALSE}
BatchGradientDescentG = function(var, f, fderivadas, mu, epsilon,limit = Inf) {
  y <- var
  dim(y) <- dim(var)

  iteraciones <- 0

  grafico <- matrix(c(iteraciones,eval(f)), nrow = 1, ncol = 2, byrow = T)
  
  while(eval(f) > epsilon && iteraciones < limit) {
    for(i in 1:dim(var)) {
      y[i] <- var[i] - mu * eval(fderivadas[i])
    }

    var <- y
    
    grafico <- rbind(grafico, c(iteraciones,eval(f)))
    iteraciones <- iteraciones + 1
  }
  
  list(y,iteraciones,grafico)
}
```

Primero aplicamos el gradiente descendente con $\eta = 0.01$
``` {r echo=FALSE}
var <- c(1,1)
dim(var) <- 2
mu <- 0.01
epsilon <- -Inf
limit <- 50

res <- BatchGradientDescentG(var, f, fderivadas, mu, epsilon, limit)

graficos <- res[[3]]

plot(graficos, xlab = "Iteraciones", ylab = "Valor")
```

Ahora lo aplicamos con $\eta = 0.1$
``` {r echo=FALSE}
punto_inicio <- c(1,1)
mu <- 0.1
epsilon <- -Inf
limit <- 50

res <- BatchGradientDescentG(punto_inicio, f, fu, fv, mu, epsilon, limit)
u <- res[1]
v <- res[2]

graficos <- res[[4]]

plot(graficos, xlab = "Iteraciones", ylab = "Valor")
```


b)  Obtener el valor mínimo y los valores de las variables (x, y) en donde se alcanzan cuando el punto de inicio se fija: (2,1, -2,1), (3, -3),(1,5, 1,5),(1, -1). Generar una tabla con los valores obtenidos

```{r echo=FALSE}
punto_inicio <- c(2.1,-2.1)
mu <- 0.01
epsilon <- -10**-2

res <- BatchGradientDescent(punto_inicio, f, fu, fv, mu, epsilon)
u <- res[1]
v <- res[2]
resultados <- matrix(c(u,v,eval(f)), ncol = 3, byrow = T)

punto_inicio <- c(3,-3)
res <- BatchGradientDescent(punto_inicio, f, fu, fv, mu, epsilon)
u <- res[1]
v <- res[2]
resultados <- rbind(resultados,c(u,v,eval(f)))

punto_inicio <- c(1.5,1.5)
res <- BatchGradientDescent(punto_inicio, f, fu, fv, mu, epsilon)
u <- res[1]
v <- res[2]
resultados <- rbind(resultados,c(u,v,eval(f)))

punto_inicio <- c(1,-1)
res <- BatchGradientDescent(punto_inicio, f, fu, fv, mu, epsilon)
u <- res[1]
v <- res[2]
resultados <- rbind(resultados,c(u,v,eval(f)))

dimnames(resultados) <- list(c("P(2.1,-2.1)", "P(3,-3)", "P(1.5,-1.5)", "P(1,-1)"),c("X","Y","Z"))

print(resultados)
```

**EJERCICIO 4**

4. ¿Cuál sería su conclusión sobre la verdadera dificultad de encontrar el mínimo
global de una función arbitraria?

La dificultad de hallar el mínimo con el algoritmo Gradiente Descendente es encontrar un criterio de parada de forma que no se quede iterando eternamente y que pueda encontrar un valor cercano al mínimo, es decir,

Dada una función:
$f:\Re^{n} \rightarrow \Re$

Encontrar un $\epsilon \in \Re | \forall a \in \Re^{n}, \epsilon \ge f(a)$
