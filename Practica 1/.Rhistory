datos <- matrix(c(rep(1, length(vIntensidades)),vIntensidades, vSimetrias), ncol = 3)
ajuste <- PLA_pocket(datos, vEtiquetas, 1000, c(0,0,0))
w <- ajuste[[1]]
cat(w)
plot ( x=vIntensidades, y=vSimetrias, col=vEtiquetas+3, xlab="Intensidad", ylab= "Simetria")
curve (w[1]*x + w[2], add=T )
data_unif <- simula_unif(50, 2, c(-50,50))
cat(data_unif)
plot(x=data_unif[,1], y=data_unif[,2],xlab = 'x_axis', ylab = 'y_axis')
data_unif <- simula_unif(50, 2, c(-50,50))
cat(data_unif[1,])
plot(x=data_unif[,1], y=data_unif[,2],xlab = 'x_axis', ylab = 'y_axis')
data_unif <- simula_unif(50, 2, c(-50,50))
cat(data_unif[1,])
is.matrix(data_unif)
plot(x=data_unif[,1], y=data_unif[,2],xlab = 'x_axis', ylab = 'y_axis')
BatchGradientDescent = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf) {
#el vector 'y' sera utilizado para almacenar las nuevas componentes
#Si las guardaramos directamente en 'var' alterariamos el punto a evaluar en ese momento.
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon)  {
#Recorremos cada una de las componentes independientes de la funcion
#Para cada componente de la funcion hay una derivada parcial
for(i in 1:dim(var)) {
#Evaluamos la derivada en el punto 'var' y seguimos el sentido negativo de la misma
#Si eval(fderivada[i]) < 0 la variable 'var[i]' aumentara siguiendo el sentido negativo
#Si eval(fderivada[i]) > 0 la variable 'var[i]' disminuira siguiendo el sentido negativo
#Si eval(fderivada[i]) = 0 la variable 'var[i]' habra llegado a su valor optimo
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
#Actualizamos el punto que evaluaremos
var <- y
iteraciones <- iteraciones + 1
}
list(y,iteraciones)
}
var <- c(0,0)
dim(var) <- 2
f <- expression( ( (var[1] ** 3) * (exp(var[2] - 2) )  - 4 * (var[2] ** 3) * exp(-var[1]) ) ** 2 )
fu <- expression(2 *  (var[1] ** 3 * exp(var[2] - 2) - 4*exp(-var[1])*var[2]**3) * (3*var[1]**2 * exp(var[2] - 2) + 4*exp(-var[1])*var[2]**3))
fv <- expression(2 * (var[1] ** 3 * exp(var[2] - 2) - 4*exp(-var[1])*var[2]**3) * (var[1] ** 3 * exp(var[2] - 2)  - 12*exp(-var[1])*var[2]  ** 2))
fderivadas <- c(fu,fv)
BatchGradientDescentM = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf, minimo_alcanzable) {
#el vector 'y' sera utilizado para almacenar las nuevas componentes
#Si las guardaramos directamente en 'var' alterariamos el punto a evaluar en ese momento.
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon && eval(f) > minimo_alcanzable)  {
#Recorremos cada una de las componentes independientes de la funcion
#Para cada componente de la funcion hay una derivada parcial
for(i in 1:dim(var)) {
#Evaluamos la derivada en el punto 'var' y seguimos el sentido negativo de la misma
#Si eval(fderivada[i]) < 0 la variable 'var[i]' aumentara siguiendo el sentido negativo
#Si eval(fderivada[i]) > 0 la variable 'var[i]' disminuira siguiendo el sentido negativo
#Si eval(fderivada[i]) = 0 la variable 'var[i]' habra llegado a su valor optimo, por tanto no se alterara
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
#Actualizamos el punto que evaluaremos
var <- y
iteraciones <- iteraciones + 1
}
list(y,iteraciones)
}
var <- c(1,1)
dim(var) <- 2
mu <- 0.05
minimo <- 10e-14
res <- BatchGradientDescentM(var, f, fderivadas, mu, minimo_alcanzable = minimo)
punto <- res[[1]]
iteraciones <- res[[2]]
cat("Para alcanzar el minimo de la función f, el algoritmo utilizado invierte ", iteraciones, " iteraciones")
var <- punto
cat("En el punto (",punto[1],",",punto[2],"). El valor f(u,v) en ese punto es ",eval(f))
var <- c(0,0)
dim(var) <- 2
f <- expression((var[1] - 2) **2 + 2 *(var[2] + 2) ** 2 + 2 * sin(2 *pi * var[1]) * sin(2 * pi * var[2]))
fu <- expression(2 * (var[1] - 2) + 2 * sin(2 * pi * var[2]) * cos(2 * pi * var[1]) * 2 * pi)
fv <- expression(4 * (var[2] + 2) + 2 * sin(2 * pi * var[1]) * cos(2 * pi * var[2]) * 2 * pi)
fderivadas <- c(fu,fv)
BatchGradientDescentG = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf) {
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
grafico <- matrix(c(iteraciones,eval(f)), nrow = 1, ncol = 2, byrow = T)
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon)  {
for(i in 1:dim(var)) {
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
var <- y
grafico <- rbind(grafico, c(iteraciones,eval(f)))
iteraciones <- iteraciones + 1
}
list(y,iteraciones,grafico)
}
var <- c(1,1)
dim(var) <- 2
mu <- 0.01
limit <- 50
res <- BatchGradientDescentG(var, f, fderivadas, mu, limit = 50)
graficos <- res[[3]]
plot(graficos, xlab = "Iteraciones", ylab = "Valor")
var <- c(1,1)
dim(var) <- 2
mu <- 0.1
epsilon <- Inf
res <- BatchGradientDescentG(var, f, fderivadas, mu, limit = 50)
punto <- res[[1]]
graficos <- res[[3]]
plot(graficos, xlab = "Iteraciones", ylab = "Valor")
mu <- 0.01
epsilon <- 10**-2
var <- c(2.1,-2.1)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- matrix(c(var[1],var[2],eval(f)), ncol = 3, byrow = T)
var <- c(3,-3)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
var <- c(1.5,1.5)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
var <- c(1,-1)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
dimnames(resultados) <- list(c("P(2.1,-2.1)", "P(3,-3)", "P(1.5,1.5)", "P(1,-1)"),c("X","Y","Z"))
print(resultados)
#   Esta función se encarga de generar una lista de matrices de 16*16, que contiene cada una
#   un numero (representado por su escala de grises).
generarLista <- function(D){
nDatos <- length(D$V1)
lista <- list()
for(i in 1:nDatos){
matrizActual <- D[i,]
lista[[i]] <- matrix(as.numeric(matrizActual[2:257]), nrow=16, ncol=16)
}
lista
}
#   Esta función se encarga de calcular la simetría de una matriz
fSimetria <- function(A){
A = abs(A-A[,ncol(A):1])
-sum(A)
}
#   Esta función se encarga de calcular la intensidad de una matriz utilizando la función
#   mean()
fIntensidad <- function(M){
mean(M)
}
# Esta función sirve para generar una recta a partir de un vector de 3 pesos
pasoARecta= function(w){
if(length(w)!= 3)
stop("Solo tiene sentido con 3 pesos")
a = -w[2]/w[3]
b = -w[1]/w[3]
c(a,b)
}
#   Esta función calcula la matriz inversa de una matriz que se le pase como argumento
#   utiliza la función svd(), y dado que M = X*X^t, por lo que es una matriz simétrica,
#   M = U*S*V^t, donde:
#   - U es una matriz cuyas columnas son los vectores propios de M*M^t
#   - S es una matriz diagonal cuyos elementos son los valores propios de M
#   - V es una matriz cuyas columnas son los vectores propios de M^t*M
#   Por tanto, M⁻¹ = (U*S*V^t)⁻¹ = V*S⁻¹*U^t
invertirMatrizSVD <- function(M){
svdDesc <- svd(M)
sInvCoef <- 1/(svdDesc$d)
sInv <- diag(x=sInvCoef, nrow=nrow(M), ncol=ncol(M))
V <- matrix(svdDesc$v, nrow=nrow(M))
U <- matrix(svdDesc$u, nrow=nrow(M))
mInv <- V %*% sInv %*% t(U)
mInv
}
# Esta función sirve para calcular una recta de regresión, que funciona cuando el conjunto de datos es separable
Regress_Lin <- function(datos, label){
X <- datos
# En H se calcula la matriz hat de los datos pasados como parámetro
H <- (invertirMatrizSVD(t(X) %*% X) %*% t(X))
# Usando la matriz hat calculada anteriormente, se calculan los pesos utilizando las etiquetas pasadas como parámetro
w <- H %*% t(matrix(label, nrow=1))
# También se calcula el error obtenido, de forma matricial
Error <- (1/length(label))*norm(datos%*%w-label)**2
sol <- list()
sol[[1]]<-pasoARecta(w)
sol[[2]]<-Error
sol[[3]]<-w
sol
}
# Esta función utiliza gradiente descendente estocástico (SGD) para calcular un vector de pesos a partir de unos datos y etiquetas, minimizando el error obtenido tras cada iteración
Logistic_Reg <- function(w, x, y, N, mu, epsilon=10e-10){
prev_w <- c(0,0,0)
Error <- 1.0
iter <- 1
while(Error >= epsilon && iter < 10000000){
TrainingIndex <- sample(1:length(y), N)
TrainingDataPoints <- x[TrainingIndex,]
TrainingLabelValues <- y[TrainingIndex]
prev_w <- w
Gradient <- (-TrainingLabelValues*TrainingDataPoints)/c(1+exp(TrainingLabelValues*TrainingDataPoints%*%w))
w <- w - mu*(1/N)*colSums(Gradient)
Error <- colSums(log(1+ exp(-TrainingLabelValues*TrainingDataPoints%*%w)))
iter <- iter+1
}
sol <- list()
sol[[1]] <- pasoARecta(w)
sol[[2]] <- Error
sol
}
# Se cambia el directorio de trabajo, y se carga en una variable el fichero zip.train, del que se va a obtener la matriz de 1 y 5
setwd("./datos")
BatchGradientDescent = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf) {
#el vector 'y' sera utilizado para almacenar las nuevas componentes
#Si las guardaramos directamente en 'var' alterariamos el punto a evaluar en ese momento.
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon)  {
#Recorremos cada una de las componentes independientes de la funcion
#Para cada componente de la funcion hay una derivada parcial
for(i in 1:dim(var)) {
#Evaluamos la derivada en el punto 'var' y seguimos el sentido negativo de la misma
#Si eval(fderivada[i]) < 0 la variable 'var[i]' aumentara siguiendo el sentido negativo
#Si eval(fderivada[i]) > 0 la variable 'var[i]' disminuira siguiendo el sentido negativo
#Si eval(fderivada[i]) = 0 la variable 'var[i]' habra llegado a su valor optimo
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
#Actualizamos el punto que evaluaremos
var <- y
iteraciones <- iteraciones + 1
}
list(y,iteraciones)
}
var <- c(0,0)
dim(var) <- 2
f <- expression( ( (var[1] ** 3) * (exp(var[2] - 2) )  - 4 * (var[2] ** 3) * exp(-var[1]) ) ** 2 )
fu <- expression(2 *  (var[1] ** 3 * exp(var[2] - 2) - 4*exp(-var[1])*var[2]**3) * (3*var[1]**2 * exp(var[2] - 2) + 4*exp(-var[1])*var[2]**3))
fv <- expression(2 * (var[1] ** 3 * exp(var[2] - 2) - 4*exp(-var[1])*var[2]**3) * (var[1] ** 3 * exp(var[2] - 2)  - 12*exp(-var[1])*var[2]  ** 2))
fderivadas <- c(fu,fv)
BatchGradientDescentM = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf, minimo_alcanzable) {
#el vector 'y' sera utilizado para almacenar las nuevas componentes
#Si las guardaramos directamente en 'var' alterariamos el punto a evaluar en ese momento.
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon && eval(f) > minimo_alcanzable)  {
#Recorremos cada una de las componentes independientes de la funcion
#Para cada componente de la funcion hay una derivada parcial
for(i in 1:dim(var)) {
#Evaluamos la derivada en el punto 'var' y seguimos el sentido negativo de la misma
#Si eval(fderivada[i]) < 0 la variable 'var[i]' aumentara siguiendo el sentido negativo
#Si eval(fderivada[i]) > 0 la variable 'var[i]' disminuira siguiendo el sentido negativo
#Si eval(fderivada[i]) = 0 la variable 'var[i]' habra llegado a su valor optimo, por tanto no se alterara
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
#Actualizamos el punto que evaluaremos
var <- y
iteraciones <- iteraciones + 1
}
list(y,iteraciones)
}
var <- c(1,1)
dim(var) <- 2
mu <- 0.05
minimo <- 10e-14
res <- BatchGradientDescentM(var, f, fderivadas, mu, minimo_alcanzable = minimo)
punto <- res[[1]]
iteraciones <- res[[2]]
cat("Para alcanzar el minimo de la función f, el algoritmo utilizado invierte ", iteraciones, " iteraciones")
var <- punto
cat("En el punto (",punto[1],",",punto[2],"). El valor f(u,v) en ese punto es ",eval(f))
var <- c(0,0)
dim(var) <- 2
f <- expression((var[1] - 2) **2 + 2 *(var[2] + 2) ** 2 + 2 * sin(2 *pi * var[1]) * sin(2 * pi * var[2]))
fu <- expression(2 * (var[1] - 2) + 2 * sin(2 * pi * var[2]) * cos(2 * pi * var[1]) * 2 * pi)
fv <- expression(4 * (var[2] + 2) + 2 * sin(2 * pi * var[1]) * cos(2 * pi * var[2]) * 2 * pi)
fderivadas <- c(fu,fv)
BatchGradientDescentG = function(var, f, fderivadas, mu, epsilon = -Inf, limit = Inf) {
y <- var
dim(y) <- dim(var)
iteraciones <- 0
eval_anterior <- Inf
grafico <- matrix(c(iteraciones,eval(f)), nrow = 1, ncol = 2, byrow = T)
while(iteraciones < limit && abs(eval_anterior - eval(f)) >= epsilon)  {
for(i in 1:dim(var)) {
y[i] <- var[i] - mu * eval(fderivadas[i])
}
eval_anterior <- eval(f)
var <- y
grafico <- rbind(grafico, c(iteraciones,eval(f)))
iteraciones <- iteraciones + 1
}
list(y,iteraciones,grafico)
}
var <- c(1,1)
dim(var) <- 2
mu <- 0.01
limit <- 50
res <- BatchGradientDescentG(var, f, fderivadas, mu, limit = 50)
graficos <- res[[3]]
plot(graficos, xlab = "Iteraciones", ylab = "Valor")
var <- c(1,1)
dim(var) <- 2
mu <- 0.1
epsilon <- Inf
res <- BatchGradientDescentG(var, f, fderivadas, mu, limit = 50)
punto <- res[[1]]
graficos <- res[[3]]
plot(graficos, xlab = "Iteraciones", ylab = "Valor")
mu <- 0.01
epsilon <- 10**-2
var <- c(2.1,-2.1)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- matrix(c(var[1],var[2],eval(f)), ncol = 3, byrow = T)
var <- c(3,-3)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
var <- c(1.5,1.5)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
var <- c(1,-1)
dim(var) <- 2
res <- BatchGradientDescent(var, f, fderivadas, mu, epsilon)
var <- res[[1]]
resultados <- rbind(resultados, c(var[1],var[2],eval(f)))
dimnames(resultados) <- list(c("P(2.1,-2.1)", "P(3,-3)", "P(1.5,1.5)", "P(1,-1)"),c("X","Y","Z"))
print(resultados)
#   Esta función se encarga de generar una lista de matrices de 16*16, que contiene cada una
#   un numero (representado por su escala de grises).
generarLista <- function(D){
nDatos <- length(D$V1)
lista <- list()
for(i in 1:nDatos){
matrizActual <- D[i,]
lista[[i]] <- matrix(as.numeric(matrizActual[2:257]), nrow=16, ncol=16)
}
lista
}
#   Esta función se encarga de calcular la simetría de una matriz
fSimetria <- function(A){
A = abs(A-A[,ncol(A):1])
-sum(A)
}
#   Esta función se encarga de calcular la intensidad de una matriz utilizando la función
#   mean()
fIntensidad <- function(M){
mean(M)
}
# Esta función sirve para generar una recta a partir de un vector de 3 pesos
pasoARecta= function(w){
if(length(w)!= 3)
stop("Solo tiene sentido con 3 pesos")
a = -w[2]/w[3]
b = -w[1]/w[3]
c(a,b)
}
#   Esta función calcula la matriz inversa de una matriz que se le pase como argumento
#   utiliza la función svd(), y dado que M = X*X^t, por lo que es una matriz simétrica,
#   M = U*S*V^t, donde:
#   - U es una matriz cuyas columnas son los vectores propios de M*M^t
#   - S es una matriz diagonal cuyos elementos son los valores propios de M
#   - V es una matriz cuyas columnas son los vectores propios de M^t*M
#   Por tanto, M⁻¹ = (U*S*V^t)⁻¹ = V*S⁻¹*U^t
invertirMatrizSVD <- function(M){
svdDesc <- svd(M)
sInvCoef <- 1/(svdDesc$d)
sInv <- diag(x=sInvCoef, nrow=nrow(M), ncol=ncol(M))
V <- matrix(svdDesc$v, nrow=nrow(M))
U <- matrix(svdDesc$u, nrow=nrow(M))
mInv <- V %*% sInv %*% t(U)
mInv
}
# Esta función sirve para calcular una recta de regresión, que funciona cuando el conjunto de datos es separable
Regress_Lin <- function(datos, label){
X <- datos
# En H se calcula la matriz hat de los datos pasados como parámetro
H <- (invertirMatrizSVD(t(X) %*% X) %*% t(X))
# Usando la matriz hat calculada anteriormente, se calculan los pesos utilizando las etiquetas pasadas como parámetro
w <- H %*% t(matrix(label, nrow=1))
# También se calcula el error obtenido, de forma matricial
Error <- (1/length(label))*norm(datos%*%w-label)**2
sol <- list()
sol[[1]]<-pasoARecta(w)
sol[[2]]<-Error
sol[[3]]<-w
sol
}
# Esta función utiliza gradiente descendente estocástico (SGD) para calcular un vector de pesos a partir de unos datos y etiquetas, minimizando el error obtenido tras cada iteración
Logistic_Reg <- function(w, x, y, N, mu, epsilon=10e-10){
prev_w <- c(0,0,0)
Error <- 1.0
iter <- 1
while(Error >= epsilon && iter < 10000000){
TrainingIndex <- sample(1:length(y), N)
TrainingDataPoints <- x[TrainingIndex,]
TrainingLabelValues <- y[TrainingIndex]
prev_w <- w
Gradient <- (-TrainingLabelValues*TrainingDataPoints)/c(1+exp(TrainingLabelValues*TrainingDataPoints%*%w))
w <- w - mu*(1/N)*colSums(Gradient)
Error <- colSums(log(1+ exp(-TrainingLabelValues*TrainingDataPoints%*%w)))
iter <- iter+1
}
sol <- list()
sol[[1]] <- pasoARecta(w)
sol[[2]] <- Error
sol
}
# Se cambia el directorio de trabajo, y se carga en una variable el fichero zip.train, del que se va a obtener la matriz de 1 y 5
setwd("./datos")
digit.train <- read.table("zip.train", quote="\"", comment.char="", stringsAsFactors=FALSE)
digitos15.train <- digit.train[digit.train$V1==1 | digit.train$V1==5,]
lista15 <- generarLista(digitos15.train)
#   Se crea un vector de simetrías, que se obtienen al utilizar la funcion fsimetria
vSimetrias <- c()
for(i in 1:length(lista15)){
vSimetrias[i] <- fSimetria(lista15[[i]])
}
#   Se crea un vector de simetrias, que se obtienen al utilizar la función fintensidad
vIntensidades <- c()
for(i in 1:length(lista15)){
vIntensidades[i] <- fIntensidad(lista15[[i]])
}
#   Se obtienen las etiquetas de los 1 y 5, modificando la de 1 a -1 y la de 5 a 1, para poder utilizar la regresion lineal
#   Con las simetrias e intensidades se crea la matriz de datos, y se calcula la regresion lineal con esta matriz y las etiquetas
#   Antes de llamar a la función, es necesario añadir una columna de 1s a la matriz datos
vEtiquetas <- digitos15.train$V1
vEtiquetas[vEtiquetas==5] <- -1
datos <- matrix(c(rep(1, length(vIntensidades)),vIntensidades, vSimetrias), ncol = 3)
w <- Regress_Lin(datos, vEtiquetas)
plot ( x=vIntensidades, y=vSimetrias, col=vEtiquetas+3, xlab="Intensidad", ylab= "Simetria")
curve (w[[1]][1]*x + w[[1]][2], add=T )
w1=c(0,0,0)
dim(w1) <- 3
#   Con gradiente descendente estocástico (SGD) obtenemos el valor de Ein y otro vector de pesos
wSGD <- Logistic_Reg(w1, datos, vEtiquetas, 50, mu=0.05)
plot ( x=vIntensidades, y=vSimetrias, col=vEtiquetas+3, xlab="Intensidad", ylab= "Simetria")
curve (wSGD[[1]][1]*x + wSGD[[1]][2], add=T )
Ein <- w[[2]]
cat("Ein vale ",Ein,"\n")
#   Ahora se calcula Eout utilizando los datos del archivo zip.test, siguiendo los pasos realizados anteriormente con zip.train
digit.test <- read.table("zip.test", quote="\"", comment.char="", stringsAsFactors=FALSE)
digitos15.test <- digit.test[digit.test$V1==1 | digit.test$V1==5,]
lista15.test <- generarLista(digitos15.test)
vSimetrias <- c()
for(i in 1:length(lista15.test)){
vSimetrias[i] <- fSimetria(lista15.test[[i]])
}
vIntensidades <- c()
for(i in 1:length(lista15.test)){
vIntensidades[i] <- fIntensidad(lista15.test[[i]])
}
vEtiquetas <- digitos15.test$V1
vEtiquetas[vEtiquetas==5] <- -1
datos <- matrix(c(rep(1, length(vIntensidades)),vIntensidades, vSimetrias), ncol = 3)
Eout <- 1/length(vEtiquetas)*norm(datos%*%w[[3]]-vEtiquetas)**2
cat("Eout vale ", Eout)
#  Función que calcula N coordenadas uniformemente muestreadas dentro del cuadrado definido por rango
simula_unif = function (N=2,dims=2, rango = c(0,1)){
m = matrix(runif(N*dims, min=rango[1], max=rango[2]),
nrow = N, ncol=dims, byrow=T)
m
}
set.seed(3)
muestra <- simula_unif(N=1000,rango=c(-1,1))
plot ( x=muestra[,1], y=muestra[,2])
#   Función utilizada para etiquetar la muestra obtenida en el ejercicio anterior
f <- function(x1, x2){
s <- ((x1-0.2)**2 + x2**2- 0.6)
if(s == abs(s)) return(1)
return(-1)
}
#   Se provoca que un 10% de las etiquetas se modifiquen, para generar ruido en la muestra obtenida
indiceRuido <-sample(1:1000, 1000*0.1)
etiquetasMuestra <- c()
for(i in 1:length(muestra[,1]))
etiquetasMuestra[i] <- f(muestra[i,1], muestra[i,2])
for(i in 1:length(indiceRuido)){
if((etiquetasMuestra[indiceRuido[i]]) == 1){
etiquetasMuestra[indiceRuido[i]] <- -1
} else{
etiquetasMuestra[indiceRuido[i]] <- 1
}
}
plot ( x=muestra[,1], y=muestra[,2], col=etiquetasMuestra+3)
datos <- matrix(c(rep(1, length(muestra[,1])), muestra[,1], muestra[,2]), ncol = 3)
#   Se calculan los pesos utilizando la función Regress_Lin
w <- Regress_Lin(datos, etiquetasMuestra)[[1]]
#   Se calcula el error utilizando gradiente descendente estocasstico (SGD)
listaSGD <- Logistic_Reg(c(0,0,0), datos, etiquetasMuestra, 100, 0.05)
